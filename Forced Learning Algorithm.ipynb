{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90e3802",
   "metadata": {},
   "source": [
    "<h2>What is this?</h2>\n",
    "\n",
    "This is research I am currently performing with the supervision of Dr. Kutz, Professor of Applied Mathematics at the University of Washington, on an algorithm to train deep neural networks. I originally devised it to stave off the degradation issue; however, it has proved itself, in preliminary results, to be just as good, if not superior, to the traditional learning algorithms used to train neural networks. I call it “Forced Learning”, due to the way that the network is trained in blocks, and each block passes its output to the next block, instead of relying on one backpropagation pass to train the entire network. This allows each backpropagation step to be shorter, thus mitigating the degradation issue, and error in backpropagation values noted in https://arxiv.org/pdf/1805.01078.pdf.\n",
    "\n",
    "I have also preliminarily noted a speed up in the amount of wall time and the number of epochs the forced learning network takes to converge, compared to a network trained using standard backpropagation. While I am still researching what causes this speed up in the amount, I believe that it might be due to the time complexity of backpropagation being polynomial, which would allow splitting up the training step to increase the total time spent on backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b458c",
   "metadata": {},
   "source": [
    "<h2>Code:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
    "\n",
    "x_test, x_val = np.array_split(x_test, 2)\n",
    "y_test, y_val = np.array_split(y_test, 2)\n",
    "\n",
    "assert x_train.shape == (50000, 32, 32, 3)\n",
    "assert x_val.shape == (5000, 32, 32, 3)\n",
    "assert x_test.shape == (5000, 32, 32, 3)\n",
    "\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_val.shape == (5000, 1)\n",
    "assert y_test.shape == (5000, 1)\n",
    "\n",
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "def get_run_logdir(model_name):\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\") + \" \" + model_name\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7491c2d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_onehot = tf.one_hot(np.squeeze(y_train), 100)\n",
    "y_val_onehot = tf.one_hot(np.squeeze(y_val), 100)\n",
    "y_test_onehot = tf.one_hot(np.squeeze(y_test), 100)\n",
    "\n",
    "x_train = x_train.astype(\"float32\")/255\n",
    "x_val = x_val.astype(\"float32\")/255\n",
    "x_test = x_test.astype(\"float32\")/255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train_onehot))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test_onehot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val_onehot))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(1)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(1)\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(32,32,3)),\n",
    "        tf.keras.layers.Conv2D(16, 3, padding='same',activation='relu'),\n",
    "        tf.keras.layers.Conv2D(16, 3, padding='same',activation='relu'),\n",
    "        tf.keras.layers.Conv2D(16, 3, padding='same',activation='relu'),\n",
    "        tf.keras.layers.Conv2D(16, 3, padding='same',activation='relu'),\n",
    "        tf.keras.layers.Conv2D(32, 3, strides=(2,2), padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, 3, strides=(2,2), padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.AveragePooling2D(pool_size=(8,8), strides=(1, 1), padding='valid'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(100, activation=\"softmax\")\n",
    "    ], name=\"mini_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018245b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_logdir = get_run_logdir(\"MiniModelVanilla_Cifar100\")\n",
    "callback = [tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=7),\n",
    "            tf.keras.callbacks.TensorBoard(run_logdir)]\n",
    "\n",
    "metrics = [tf.keras.metrics.MeanAbsoluteError(name=\"mae\"),\n",
    "            tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\")]\n",
    "\n",
    "mini_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), metrics=metrics)\n",
    "\n",
    "history = mini_model.fit(train_dataset, epochs=200, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c91acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_1 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(16, 3, padding='same',activation='relu'),\n",
    "        tf.keras.layers.Conv2D(16, 3, padding='same',activation='relu'),\n",
    "        tf.keras.layers.Conv2D(16, 3, padding='same',activation='relu'),\n",
    "        tf.keras.layers.Conv2D(16, 3, padding='same',activation='relu'),\n",
    "        tf.keras.layers.Conv2D(32, 3, strides=(2,2), padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation='relu'),\n",
    "    ], name=\"small_1\"\n",
    ")\n",
    "\n",
    "auxillary = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.AveragePooling2D(pool_size=(8,8), strides=(1, 1), padding='valid'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(100, activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "inputs = tf.keras.Input(shape=(32,32,3))\n",
    "x = small_1(inputs)\n",
    "outputs = auxillary(x)\n",
    "\n",
    "small_model_pretrain = tf.keras.Model(inputs=inputs, outputs=outputs,name=\"small_model_pretrain\")\n",
    "\n",
    "small_2 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(16,16,32)),\n",
    "        tf.keras.layers.Conv2D(64, 3, strides=(2,2), padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation='relu'),\n",
    "        tf.keras.layers.AveragePooling2D(pool_size=(8,8), strides=(1, 1), padding='valid'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(100, activation=\"softmax\")\n",
    "    ], name=\"small_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_metric_1 = tf.keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "accuracy_metric_1 = tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\")\n",
    "\n",
    "mae_metric_2 = tf.keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "accuracy_metric_2 = tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\")\n",
    "\n",
    "class ForcedNetSmall(tf.keras.Model):\n",
    "    def __init__(self, block_1, block_2, auxillary):\n",
    "        super(ForcedNetSmall, self).__init__()\n",
    "        \n",
    "        inputs = tf.keras.Input(shape=(32,32,3))\n",
    "        \n",
    "        self.auxillary_1 = auxillary\n",
    "        \n",
    "        self.model_1 = block_1\n",
    "        \n",
    "        self.model_2 = block_2\n",
    "\n",
    "        \n",
    "    def compile(self, optimizer, loss_fn):\n",
    "        super(ForcedNetSmall, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [mae_metric_1, accuracy_metric_1, \n",
    "                mae_metric_2, accuracy_metric_2]\n",
    "        \n",
    "    def call(self, images):\n",
    "        x = self.block_1(images)\n",
    "        x = self.block_2(x)\n",
    "        return self.block_3(x)\n",
    "    \n",
    "    def summary(self):\n",
    "        self.model_1.summary()\n",
    "        self.model_2.summary()\n",
    "        print(\"\\nAuxillary Layers:\")\n",
    "        self.auxillary_1.summary()\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.model_1(images)\n",
    "        predictions_1 = self.auxillary_1(x)\n",
    "        \n",
    "        \n",
    "        #Second model part\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            x = self.model_2(images)\n",
    "            predictions_2 = self.auxillary_2(x)\n",
    "            \n",
    "            loss_2 = self.loss_fn(labels, predictions_2)\n",
    "            \n",
    "        grads = tape.gradient(loss_2, self.model_2.trainable_weights)\n",
    "        grads_output = tape.gradient(loss_2, self.auxillary_2.trainable_weights)\n",
    "        \n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model_2.trainable_weights,)\n",
    "        )\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads_output, self.auxillary_2.trainable_weights)\n",
    "        )\n",
    "        \n",
    "        mae_metric_1.update_state(labels, predictions_1)\n",
    "        accuracy_metric_1.update_state(labels, predictions_1) \n",
    "        \n",
    "        mae_metric_2.update_state(labels, predictions_2)\n",
    "        accuracy_metric_2.update_state(labels, predictions_2)\n",
    "        \n",
    "        return {\"Block_1_Loss\": loss_1,\n",
    "                \"Block_2_Loss\": loss_2,\n",
    "                \n",
    "                \"Block_1_MAE\": mae_metric_1.result(),\n",
    "                \"Block_2_MAE\": mae_metric_2.result(), \n",
    "                \n",
    "                \"Block_1_Accuracy\": accuracy_metric_1.result(),\n",
    "                \"Block_2_Accuracy\": accuracy_metric_2.result(), }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c7b20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = [tf.keras.metrics.MeanAbsoluteError(name=\"mae\"),\n",
    "            tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\")]\n",
    "\n",
    "run_logdir = get_run_logdir(\"SmallForced_pretrain_Cifar100\")\n",
    "\n",
    "callback = [tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=7), tf.keras.callbacks.TensorBoard(run_logdir)]\n",
    "small_model_pretrain.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003), metrics=[metrics]\n",
    ")\n",
    "\n",
    "history = small_model_pretrain.fit(train_dataset, epochs=125, callbacks=[callback])\n",
    "\n",
    "run_logdir = get_run_logdir(\"SmallForcedLearner_Cifar100\")\n",
    "\n",
    "ForcedSmall = ForcedNetSmall(small_1, small_2, auxillary)\n",
    "\n",
    "callback = [tf.keras.callbacks.ReduceLROnPlateau(monitor='Block_2_Loss', patience=7), tf.keras.callbacks.TensorBoard(run_logdir)]\n",
    "ForcedSmall.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),\n",
    "    loss_fn=tf.keras.losses.CategoricalCrossentropy(),\n",
    ")\n",
    "\n",
    "history = ForcedSmall.fit(train_dataset, epochs=75, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5053be",
   "metadata": {},
   "source": [
    "<h2>Log 1: Algorithm Change</h2>\n",
    "\n",
    "As can be seen from runs prior to run_2021_12_29-00_26_20 ForcedLearning20Cifar100Adam, the categorical\n",
    "accuracy progresses much more slowly than the vanilla variant of the model. I hypothesize that this is\n",
    "because the model has to \"Catch-up\" with itself, because each block is learning independently. This\n",
    "causes each successive block in the model to spend the next batch adjusting to what the previous batch just\n",
    "learned, and not gaining any intelligence. I have rectified this by allowing each training step to have the\n",
    "model pass through one layer, update that specific model_block, pass through another, update that block wrt\n",
    "the first and second block, and so on. This has increased the time, but vastly increased the speed at which\n",
    "the model learns, even surpassing the vanilla model at the beginning of the training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
